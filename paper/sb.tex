\documentclass{article} % For LaTeX2e
\usepackage{nips12submit_e,times}
%\documentstyle[nips12submit_09,times,art10]{article} % For LaTeX 2.09

\usepackage{amsmath,amssymb,amsthm,amsfonts,comment}
\usepackage{graphicx}\graphicspath{{figures/}}
\usepackage[small,labelfont=bf]{caption}
\usepackage[square]{natbib}
\usepackage{color}
\usepackage{subfigure}
\usepackage{epstopdf}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage{multirow} 

%% our math environment
\def\[#1\]{\begin{align}#1\end{align}}

\newcommand{\defn}[1]{{\bf #1}}

\newcommand{\defas}{:=}
\newcommand{\given}{\mid}

\newcommand{\Naturals}{\mathbb{N}}
\newcommand{\Rationals}{\mathbb{Q}}
\newcommand{\Reals}{\mathbb{R}}
\newcommand{\cS}{\mathcal{S}}
\newcommand{\st}{\,:\,}
\newcommand{\RInts}{\mathcal{I}_\Rationals}
\newcommand{\BSets}{\mathcal{B}_\Reals}
\newcommand{\grad}{\bigtriangledown}
\definecolor{LimeGreen}{rgb}{0.1,0.9,0.1}
\definecolor{Maroon}{rgb}{0.9,0.1,0.1}

%% our math environment
 
\newtheorem{thm}{Theorem}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lem}[thm]{Lemma}
%\newtheorem{definition}{Definition}

\title{Optimal independence tests for Bayesian Networks}

\author{
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
(if needed)\\
}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\nipsfinalcopy % Uncomment for camera-ready version

\begin{document}


\maketitle

\begin{abstract}

\end{abstract}


\section{Introduction}
Learning Bayesian networks for general distributions is
intractable task \cite{chickering1996learning}. However, often
for real data distributions, still we are able to 
recover Bayesian network structure. This implies that real data
distribution has some special properties, which simplify process
of structure recovery. Such process is almost always
based on computation local statistics, and then 
reasoning about global structure \cite{jaakkola2010learning, tsamardinos2006max}. 


Such local statistics can describe
complexity of network (e.g. number of parameters in case of BIC\cite{schwarz1978estimating}), or
can measure dependency of between nodes (e.g. mutual information tests, 
conditional independence tests). 
We focus in this work on how to best measure dependency between nodes
in empirical CPDs from Bayessian network. We are mainly interest
in developing techniques applicable to discovery of structure
in gene expression data. This implies that each random variable
can belong to many classes (when expression is quantized), or have continuous
values. Moreover, such datasets are always relatively small $\sim200$ samples.
We focus our attention on addressing Bayesian network structure learning
in aforementioned regime. 


Our main contribution is in designing very efficient dependency test
for Bayesian networks. It is kernelized partial correlation with
kernels sensitive to linear dependency present in CPDs. We show
that kernelized partial correlation with proper kernel in ``gene expression''
regime is almost always better than any other previously considered
dependency test. 


\section{Related work}
Independence tests used in Bayesian networks
\begin{itemize}
\item \cite{schafer2005empirical} Here they learn a Gaussian Graphical Model (GGM) using estimates of the partial correlation matrix. 
\item \cite{opgen2007correlation} Here they learn an approximate causal structure on gene expression based on full-order partial correlation (as an approximation to lower-order partial correlation that is called for theoretically for a Bayesian network).
\item \cite{tsamardinos2006max} MMHC algorithm.  They use a test based on what is called the $G^2$ statistic (asymptotically distributed as $chi^2$) and they also talk about a couple other independence tests that we may want to look into.
\end{itemize}

There has been extensive research in area of scoring functions for
Bayesian networks. This step is crucial to prevent algorithms from 
recovering complete graph structure.
Without any scoring function, log-likelihood term would force optimization
to choose fully connected graph. There have been proposed few regularizations (scoring functions)
to address this problem. There are two general types of scoring functions 
(1) based purely on model complexity, (2) combining model complexity with data evidences.



To the first group belongs 
one most widely used scoring functions, which 
is Bayesian Information Criterion (BIC) \cite{schwarz1978estimating}.
There are couple of others like Hannanâ€“Quinn information criterion (HQC) \cite{hannan1979determination},
or Bayesian model comparison (BMC). Major drawback of scoring functions based
purely on model complexity is their constant power regardless of amount of data.
Even if data speaks strongly about dependency, such scoring functions won't take
it into account.




Variety of such scoring functions calculate dependency between nodes conditioned on
potential parents \cite{de2006scoring}. Usual measures of dependency are
based on mutual information, conditional independence test, or 
are fully Bayesian. Fully Bayesian methods assume probability 
distribution over CPDs of independent variables,
and dependent variables.




We should discuss:
\begin{itemize}
\item LL (Log-likelihood) (1912-22)
\item MDL/BIC (Minimum description length/Bayesian Information Criterion) (1978)
\item AIC (Akaike Information Criterion) (1974)
\item NML (Normalized Minimum Likelihood) (2008)
\item MIT (Mutual Information Tests) (2006)
\end{itemize}
Use in Bayesian networks \cite{schafer2005empirical}

Existing independence tests:
\begin{itemize}
\item Pearson's $\chi$-squared.  The problem is the null hypothesis is independence, but independence is what we're trying to show.
\end{itemize}

\cite{margaritis2003learning}

\section{Independence testing} 
Independence tests decides on independence of random variables. More formally,
for a random variables $A, B$ given $C$, test decides on equality $P(A, B| C) = P(A | C) P(B | C)$. 
Figure \ref{fig:ind} presents surface of $2$ dimensional independent 
random variables $A, B$, and $C = \emptyset$. Independence choses if samples $a \in A, b \in B, c \in C$, 
are coming from independence surface, or not. 
This problem can be casted as a classification problem, and we define
empirical loss for it as:
\begin{equation}
  \mathbb{E}_{a \in A, b \in B, c \in C, y \in \{\text{indepedent}, \text{depedent}\}} L(f((a, b)|c, y)
\end{equation}
Our initial task is to find $f$, which well predicts independence given our prior
knowledge on $A, B, C$. Such task is not easy as the manifold of independent
random variables is highly non-linear. We review several approaches, and 
settle on kernel independence tests, which gives superior performance on
samples drawn from Bayesian networks.


There are few issues, which such test has to address 
(1) account for prior over $A, B, C$ \ref{sec:prior}, (2) diminishing number of samples when
counting occurrences \ref{sec:curse}. We discuss this issues
in following sections.


\begin{figure}[h]
\centering
\includegraphics[width=0.55\linewidth]{img/independence_surface.eps}
\caption{The manifold of independence for binary distributions. The simplex represents all possible joint distributions over two binary variables parametrized with their marginal distributions. The simplex is formed by the constraint that marginal distributions must be positive and sum to one.  The manifold corresponds to the set (of measure zero) of independent distributions.}
\label{fig:ind}
\end{figure}

\subsection{Prior}\label{sec:prior}
Set $(A, B, C, y=\text{independent})$ has measure zero for uniform
distribution over CPDs of $A, B, C$. For such distribution
best possible predictor $f$ would always predict ``dependence''. 
This shows that a prior coming from any natural parametrization
of CPDs would force $f$ to be constant. This indicate that, we should
employ better prior. We are interest in non-parametric priors over $(A, B, C)$.
Moreover, we would like our test to be consistent.


In gene expression data, random variables (gene expressions) can 
have arbitrary real values. Common practise is to quantize such values,
and count number of values in every bucket. However, this removes 
''smoothness`` prior over assignments. ``Smoothness'' prior means that
nearby values of random variables have a similar meaning. We can easily see
that common measure of dependency such as mutual information {\bf don't} have 
this property. 
\begin{align*}
  I(A;B)=\sum_{a \in A}\sum_{b \in B} p(a, b)\log{\frac{p(a, b)}{p(a)p(b)}}=I(\sigma_A(A), \sigma_B(B))
\end{align*}
$\sigma_A$ is a permutation over values of $A$, and $\sigma_B$ is
a permutation over values of $B$.



Another prevalent measure of dependency is correlation. This measure
has a notion of ``smoothness'' (it treats near by values similarly), but
it is not consistent. Moreover, empirical experiments show that zero value
of correlation often doesn't imply lack of dependency.


\subsection{Curse of conditioning}\label{sec:curse}

\subsection{Partial correlation}

\subsection{Kernelized partial correlation}


\begin{table}[t]
\centering
\tiny
\begin{tabular}{rrrr}
\hline
Measure & Consistent & Smooth prior& Value based\\
& & & conditioning\\
\hline
Mutual information & $\color{LimeGreen} \checkmark $ & $\color{Maroon}\times$ & $\color{Maroon}\times$ \\
Correlation & $\color{Maroon}\times$  & $\color{LimeGreen} \checkmark $ & $\color{Maroon}\times$ \\
Partial correlation & $\color{Maroon}\times$  & $\color{LimeGreen} \checkmark $ & $\color{LimeGreen}\checkmark$ \\
KCI for characteristic kernel & $\color{LimeGreen}\checkmark$  & $\color{LimeGreen} \checkmark $ & $\color{LimeGreen}\checkmark$ \\
\hline
\end{tabular}
\caption{Comparison of properties for various independence tests.}
\label{eval}
\end{table}


\section{Experiments}

\subsection{Classification of Synthetic CPDs}
We generated distributions from toy Bayesian networks.

\begin{figure}[h]
\centering
\subfigure{
  \includegraphics[width=0.45\linewidth]{img/roc_full_random_arity5.eps}
}
\subfigure{
\includegraphics[width=0.45\linewidth]{img/roc_partial_random_arity5.eps}
}
\caption{Precision-recall curves for various classifiers. Plots present
results for asia network, with nodes having 5 possible classes. Local CPDs
have been chosen to express linear relation. {\bf (Left)} Entire 
precision-recall curve, {\bf (Right)} Low recall fragment of precision-recall curve.}

\end{figure}

\subsection{Classification of CPDs from Gene Expression}

\subsection{Synthetic Bayesian networks}

\subsection{Gene expression data}

\section{Discussion}

\begin{small}
%\renewcommand\bibname{References}
\bibliographystyle{abbrvnat}
%\bibliographystyle{authordate1}
%\bibliographystyle{amsnomr}
\bibliography{bibliography}
\end{small}

%\appendix
%\include{appendix}

\end{document}
